\newcommand{\bSigma}{\ensuremath{\boldsymbol{\varSigma}}}
\newcommand{\bLambda}{\ensuremath{\boldsymbol{\varLambda}}}
\renewenvironment{Schunk}{\small}{}
<<echo=false,results=hide>>=
require(barebone)
require(vegan)
options(width=66)
@
\chapter{R Programming Interface to Multivariate Analysis}
\label{sec:basicmethods}

Short intro: the scope of this Appendix.

\section{Matrix Algebra in R}

\subsection{Basic Operations}

\subsection{Matrix Decomposition}

\section{Simulating Simple Communities}

Simulated communities are useful in studying the properties of the
different multivariate methods. It is easier to understand and interpret
the results if we know the truth,
i.e.~assumptions and parameters under which the community was generated.

The simplest thing one can do is to generate abundances for $i = 1,2,\ldots,n$
sites and for $j = 1,2,\ldots,m$ independent species based on random
numbers from Poisson distribution:
\begin{eqnarray}
  y_{ij} &\sim &\mathrm{Poisson}(\lambda_{ij})\\
  \log(\lambda_{ij}) &=& x_i \beta_j\;,
\end{eqnarray}
where $x_i$ is the value the independent variable in row $i$, and
$\beta_j$ is the regression coefficient for column (species) $j$.

This is done in
<<echo=false>>=
dump("rspecies", file = "")
@
This function returns an \texttt{n} by \texttt{spp} matrix of species abundances.
The mean abundances are determined by \texttt{b} values of the intercept
<<>>=
rspecies(n=5, spp=2, b=c(2,4))
@
Other parameters can be added to model responses to environmental covariates
(note that now species responses are described elementwise in \texttt{b})
<<>>=
x <- rnorm(10)
rspecies(n=10, spp=2, b=list(c(2,1), c(1,2)), x=model.matrix(~x))
@
Covariates should be provided as model matrix, that can be done easily by the
formula interface of the \texttt{model.matrix} function.

\section{R Programming of Ordination Methods}

\subsection{Unconstrained Ordination}

\subsubsection{Principal Components Analysis (PCA)}

The simplest and numerically best method for PCA is equivalent to
singular value decomposition (SVD) of the data matrix. The SVD is
defined as
\begin{eqnarray}
  \vec{Y} &= \vec{U \bSigma V'}\;,
\end{eqnarray}
where \vec{\bSigma} is a diagonal matrix of singular values which are
square roots of eigen values $\vec{\bSigma = \bLambda}^{1/2}$, \vec{Y}
is the data matrix with centred columns, and \vec{U} and \vec{V} are
orthonormal matrices of row and column scores, so that $\vec{U'U} =
\vec{I}$ and $\vec{V'V} = \vec{I}$. The implementation is
straightforward:
<<echo=false>>=
dump("PCA", file = "")
@
The function returns a result of the R function \texttt{svd}. The only
optional argument is \texttt{scale} which defines how data is scaled
before the analysis. In general, the standard R function
\texttt{scale} transforms data as:
\begin{equation}
  \label{eq:PCAstand}
  y'_{ij} = \frac{y_{ij} - z_j}{s_j}\;,
\end{equation}
where $z_j$ is the column centre, and $s_j$ is the column
scale. Typically $z_j$ is the column average. With \texttt{scale =
  FALSE}, $s=1$ (or no scaling), and with \texttt{scale = TRUE}, $s_j$
are the variable (column) standard deviations. If data are scaled by
column standard deviations, all columns will have equal weight.  The
\texttt{scale} can also be a vector of scales. Using a constant
scale for all columns does not influence scores \vec{U} or \vec{V},
but it will influence the absolute singular values
\vec{\bSigma}---the proportions of individual elements $\sigma$ do
not change.  With the basic analysis, the singular values decompose
the original observed values, so that the sum of $\vec{\bSigma}^2$
equals the sum of $\vec{Y}^2$ for centred \vec{Y}. If we want the
singular values to decompose the variance of the data, we can call the
function as
<<>>=
data(dune)
str(PCA(dune, scale = rep(sqrt(nrow(dune)-1), ncol(dune))))
@
This divides columns with $\sqrt{n-1}$ so that the sum of
$\vec{\bSigma}^2$ equals the sum of variances of \vec{X}, and the
singular values are standard deviations of the axes.  Using scale
$\sqrt{n}$ would give the same results with biased esimate of variance.

SVD is the preferred method of performing PCA, but earlier PCA was
usually performed as eigen decomposition of symmetric crossproduct
matrix:
\begin{equation}
  \vec{Y'Y} = \vec{V \bLambda V'}\;,
\end{equation}
where \vec{\bLambda} is a diagonal matrix of eigenvalues. The row
scores can be found as
\begin{equation}
  \label{eq:PCAu}
  \vec{U} = \vec{Y' V \bLambda}^{-1/2}\;.
\end{equation}
The row and column scores \vec{U} and \vec{V} are identical to those
found in the previous methods (apart from possible sign reversals),
and $\vec{\bLambda} = \vec{\bSigma}^2$.
<<echo=false>>=
dump("PCAeig", file = "")
@
It is a common practice to omit multiplication with
$\vec{\bLambda}^{-1/2}$ in (\ref{eq:PCAu}) or, equivalently, division
by square roots of eigenvalues in \texttt{PCAeig}. In that case the
row scores \vec{U} are not orthonormal, but the sum of squares of each
column is proportional to square roots of eigenvalues. The equations
are:
\begin{eqnarray}
  \label{eq:PCAurot}
  \vec{U} &=& \vec{Y' V}\\
  \vec{U'U} &=& \vec{\bLambda}^{1/2} \;.
\end{eqnarray}
With this scaling, \vec{U} are called \emph{scores matrix}, and
orthonormal \vec{V} are called \emph{loadings} or \emph{rotation
  matrix}: Multiplication with (\ref{eq:PCAurot}) rotates the centred
and scaled data matrix \vec{Y} to the principal components.

Function \texttt{PCAeig} used crossproduct \vec{Y'Y} to get the
symmetric matrix for eigen decomposition. Most PCA software use
instead covariance matrix. This is equivalent of finding the
crossproduct of centred \vec{Y} as $ (n-1)^{-1} \vec{Y'Y}$. In this way
the eigenvalues sum up to total variance. Similarly, the scaled
solution is usually found using correlation matrix instead of explicit
scaling of response data with (\ref{eq:PCAstand}) like in SVD.

R functions are based on either of these algorithms:
\begin{itemize}
\item \texttt{prcomp} uses SVD, but it scales singular values by
  $\sqrt{n-1}$, and calls them standard deviations
  (\texttt{sdev}). Function also discards the orthonormal row scores
  \vec{U} that SVD returned, and instead uses (\ref{eq:PCAurot}) to
  find row scores scaled by eigenvalues (called \texttt{x}). The
  orthonormal column scores \vec{V} are called rotation.  The scaled
  solution is found by scaling the respone matrix with
  (\ref{eq:PCAstand}).
\item \texttt{princomp} uses eigen decomposition of covariance or
  correlation matrices. The function uses biased estimate of variance,
  so that the denominator in variance is $n$ instead of $n-1$ of
  \texttt{prcomp}. The function returns square roots of eigenvalues
  and calls them standard deviations, just like in
  \texttt{prcomp}. However, the reported eigenvalues differ from
  \texttt{prcomp} which uses unbiased variance instead of biased
  variance of \texttt{princomp}.  The function uses (\ref{eq:PCAurot})
  to find row scores \vec{U}. The row scores are called
  \texttt{scores} and orthonormal column scores are called
  \texttt{loadings}.
\item \texttt{rda} in vegan package uses SVD in its PCA. The function
  returns singular values divided with $\sqrt{n-1}$ consistently with
  \texttt{prcomp}.  Function returns orthonormal \vec{U} and
  \vec{V}. These are accessed via \texttt{scores} function  which
  scales them for
  functions like \texttt{plot} and \texttt{summary}.
  Vegan allows scaling either \vec{U} or \vec{V} by
  eigenvalues. In addition, there is a general scaling constant that
  tries to scale \vec{U} and \vec{V} so that they can be displayed in
  the same plot with equal scaling. Vegan includes a vignette on
  ``Design decisions'' which explains the scaling alternatives.
\end{itemize}

\subsubsection{Principal Coordinates Analysis}

Principal Coordinates Analysis (PCoA) is closely related to PCA, in
particular to PCA performed through spectral decomposition (function
\texttt{PCAeig}).  The PCA with spectral decomposition analyses matrix
of crossproducts and sums of squares, but PCoA analyses distances.
Actually, the only task is to convert distances into crossproducts,
and then analyse these like in \texttt{PCAeig} \citep{Mardia79}. If
the distances are Euclidean, then PCoA gives the same results as PCA,
but is less efficient.  Normally we ignore the assumption of Euclidean
distances, and use PCoA with other distances or dissimilarities that
we regard as more appropriate for our data.

The algorithm follows \citet{Mardia79}:
\begin{enumerate}
  \item Centre squared distances by rows and by columns
    \begin{equation}
      \vec{M} = \vec{H D}^2 \vec{H}\;,
    \end{equation}
    where \vec{D} are distances, and \vec{H} is a centring matrix. The
    centring matrix is defined as $\vec{H} = \vec{I}-n^{-1}\vec{11'}$,
    where $n$ in the number of rows and columns of square \vec{D},
    \vec{I} is an identity matrix, and \vec{1} is a column vector of
    ones.  The centring matrix gives a mathematically nice
    presentation of centring, but in practice we use lighter
    procedures. The example code below uses just the standard
    \texttt{scale} function, and the standard R implementation of the
    method (function \texttt{cmdscale}) has a fast C routine
    \texttt{dblcen} which is the choice in real applications.
  \item Now matrix $-\vec{M}/2$ presents the crossproducts and sum of
    squares and is submitted to spectral decomposition.
\end{enumerate}
<<echo=false>>=
dump("PCoA", file = "")
@ 

The function returns all eigenvalues and orthonormal row scores. Due
to centring, there are at most $n-1$. If \texttt{D} was non-Euclidean,
semimetric index, there will also be negative eigenvalues, so that the
number of positive eigenvalues is even lower than $n-1$. Neither zero
nor negative eigenvalues are removed in the current function, nor are
any warnings issued. If we wish to scale the row scores by
eigenvalues, we can only use the negative ones. If we want to
reconstruct the original distance matrix from its decomposition, we
must substract the crossproducts of negative eigenvalues from the
crossproduct of positive ones.

This algorithm is used in standard R function \texttt{cmdscale}.
Functions like \texttt{adonis} and \texttt{betadisper} also use
basically the same kind of translation of dissimilarities to
crossproducts.

\subsubsection{Weighted Principal Coordinate Analysis}

Any eigenvector method can be weighted, but here we only demonstrate
the weigthing for PCoA.  Similar principles also hold for PCA or RDA
(and CA always is inherently weighted).  Integer weights may only tell
how many copies of a row we have in our data set: weight 1 means a
unique row, and weight 2 indicates a duplicated row. However, the
weights can also be non-integer.

Weighting reduces or increases the influence of a point with respect
to other points. The influence of the point in unweighted ordination
is given by its distance from the centroid of the space: far points
have a higher moment. Reducing a weight means moving a point closer to
the centroid, and increasing the weight moving point further away, but
keeping the angles between points unchanged. After weigthing, we
perform the rotation via SVD or spectral decomposition. After rotation
we have to recover the original configuration and move the points back
to their original positions: weighting influences the rotation but not
the full space.  The stages of weighted ordinations are weigthing,
rotation, and de-weigthing.

Weighted PCoA has the following stages:
\begin{enumerate}
\item Let \vec{D} be a $n \times n$ distance matrix, and \vec{w} a
  corresponding weight vector of length $n$.  Centre first rows and
  then columns of this matrix using weighted centres with weights
  $\vec{w}^{1/2}$ to get a double centred matrix \vec{\bar{D}}.
\item Weight this matrix 
  \begin{equation}
    \vec{M} = \vec{W}^{1/2} \vec{\bar D} \vec{W}^{1/2}\;, 
  \end{equation}
  where \vec{W} is a diagonal matrix of \vec{w}.
\item Perform the spectral decomposition
  \begin{equation}
    -\vec{M}/2 = \vec{\tilde U} \bLambda \vec{\tilde U'}\;,
  \end{equation}
  where \vec{\tilde U} are the orthonormal row scores, and
  \vec{\bLambda} is a diagonal matrix of eigenvalues.
\item De-weight or remove the weights
  \begin{equation}
    \vec{U} = \vec{W}^{-1/2} \vec{\tilde U}\;.
  \end{equation}
\end{enumerate}

The implementation is straighforward:
<<echo=false>>=
dump("wPCoA", file = "")
@ 

\subsubsection{Correspondence Analysis}

Correspondence analysis (CA) is weighted PCA with Chi-square metrics
\citep{Nishisato80,Greenacre84, GreenacreHastie87, Greenacre07}. Just like with
PCA, we can use SVD or spectral decomposition.

The SVD algorithm is based on \citet{GreenacreHastie87}:
\begin{enumerate}
\item Let \vec{Y} be the data matrix standardized to unit sum, with
  row sums \vec{r} and column sums \vec{c}. Perform weighted Chi-square
  transformation, where a typical element $\{\bar{y}_{ij}\} =
  \vec{\bar Y}$ is
  \begin{equation}
    \label{eq:CAstand}
    \bar{y}_{ij} = \frac{y_{ij} - r_i c_j}{\sqrt{r_i c_j}} \;.
  \end{equation}
  This is obviously Chi-square transformation because by usual
  definition $\chi^2 = \sum \bar{y}_{ij}^2$.
\item Perform SVD  $\vec{\bar Y} = \vec{\tilde{U} \bSigma
    \tilde{V}'}$.
\item De-weight the solution by
  \begin{eqnarray}
    \vec{U}&=& \vec{D_r}^{-1/2} \vec{\tilde{U}}\\
    \vec{V}&=& \vec{D_c}^{-1/2} \vec{\tilde{V}} \;,
  \end{eqnarray}
  where $\vec{D_r}$ and $\vec{D_c}$ are diagonal matrices of row and
  column weights.
\end{enumerate}
Direct implementation in R is:
<<echo=false>>=
dump("CA", file = "")
@
This returns a result from SVD with row and column scores (\vec{V} and
\vec{U}) and singular values which are square roots of eigenvalues.

The spectral decomposition of a symmetric matrix follows
\citet{Nishisato80}:
\begin{enumerate}
\item Let \vec{Y} be the response matrix standardized to unit sum and
  \vec{r} and \vec{c} vectors of its row and column sums. Find the
  weighted crossproduct matrix
  \begin{equation}
    \label{eq:CAmattrans}
    \vec{\bar Y' \bar Y} = \vec{D_c}^{-1/2} \vec{Y'} \vec{D_r}^{-1} \vec{Y} \vec{D_c}^{-1/2} \;,
  \end{equation}
  where $\vec{D_c}$ and $\vec{D_r}$ are diagonal matrices of column
  and row sums.
\item Perform spectral decomposition:
  \begin{equation}
    \label{eq:CAeigen}
    \vec{\bar Y' \bar Y} = \vec{\tilde V \bLambda \tilde V'} \;.
  \end{equation}
\item De-weight the column scores:
  \begin{equation}
    \vec{V} = \vec{D_c}^{-1/2} \vec{\tilde{V}} \;.
  \end{equation}
\item Find the row scores:
  \begin{equation}
    \vec{U} = \vec{D_r}^{-1} \vec{ Y V} \vec{\bLambda}^{-1/2} \;.
  \end{equation}
\item The original transformation (\ref{eq:CAmattrans}) was not
  centred, and the first eigen vector of (\ref{eq:CAeigen}) will be
  so called trivial solution of $\lambda=1$ to do the centring omitted
  earlier. We wish to discard this first trivial axis.
\end{enumerate}
Direct implementation in R is:
<<echo=false>>=
dump("CAeig", file="")
@

\subsection{Constrained Ordination}

In constrained ordination we have constraints or environmental
variables that restrict the ordination. The two basic methods are
Redundancy Analysis which is generalized from PCA, and Constrained
Correspondence Analysis which is generalized from CA.

\subsubsection{Redundancy Analysis}

When \citet{terBraak86} introduced Constrained or Canonical
Correspondence Analysis, he also suggested an algorithm with
alternating regression and weighted averaging steps. In regression
step, the site scores are found as linear combinations of constraints
(environmental variables), and in weighted averaging step site scores
are found from species scores. Redundancy analysis is similar but
weighted averaging is replaced with weighted summation step
\citep{terBraak86, Jongman87}. \citet{Palmer93} gives a remarkably
lucid overview of this algorithm.

The algorithm of \citet{terBraak86} is often taken as the definition of
the method of constrained ordination. However, we can achieve the same
results with some other algorithms. \citet{LegLeg98} showed that a
better way of obtaining all constrained axes simultaneously is to
analyse fitted values \vec{\hat Y} of
\begin{equation}
  \label{eq:RDA.Yhat}
  \vec{\hat Y} = \vec{X} (\vec{X'X})^{-1} \vec{X'Y} \;,
\end{equation}
where \vec{Y} is the centred response matrix, and \vec{X} is the
centred model matrix of constraints (environmental variables).  The
constrained ordination solution is SVD of \vec{\hat Y}.  The model may
look tricky, but in fact it is a model of multiple linear regression
\citep{Mardia79}. In other words, Redundancy Analysis can be performed
by first fitting linear regression for each column separately using
all constraints as predictors, and then performing PCA for the fitted
values of this regression.

Direct implementation of (\ref{eq:RDA.Yhat}) would be simple in R:
<<eval=false,results=hide>>=
X <- scale(X, scale = FALSE)
Y <- scale(Y, scale = FALSE)
Yhat <- X %*% solve(t(X) %*% X) %*% t(X) %*% Y
svd(Yhat)
@
However, this is not an optimal algorithm. The problem is that
(\ref{eq:RDA.Yhat}) involves matrix inversion (\texttt{solve()}), and
the constraint matrix \vec{X} can be singular or nearly singular. With
completely singular matrices, we may use generalized inverses such as
function \texttt{ginv} in MASS \citep{MASS}, but there is a real
danger of loss of numerical precision with nearly singular
matrices. Similarly as in linear regression, it is better to use QR
decomposition \citep{BlueBook}.

The QR approach finds the row scores \vec{U} that are linear
combinations of constraints and the corresponding column scores
\vec{V}.  The alternating regression and weighted sums algorithm also
finds row scores \vec{W} that are weighted sums of column scores. In
CA framework, they are weighted averages, and consequently called
weighted average or WA scores \citep{Palmer93}. In direct regression
approach they must be estimated separately as
\begin{equation}
  \label{eq:RDA.W}
  \vec{W} = \vec{Y V \bSigma}^{-1}
\end{equation}

The analysis of \vec{\hat Y} gives the constrained part analysis. The
analysis of the residuals $\vec{Y} - \vec{\hat Y}$ gives the residual
unconstrained analysis.

Putting this all together gives the following algorithm:
\begin{enumerate}
\item Let \vec{Y} be centred response matrix, and \vec{X} centred
  model matrix of constraints.  Find QR decomposition of \vec{X}
  and fitted values \vec{\hat Y}.
\item Perform SVD of $\vec{\hat Y} = \vec{U \bSigma V'}$ with
  orthonormal column and row matrices \vec{U} and \vec{V}, and
  diagonal matrix of singular values \bSigma.
\item Find the ``WA scores'' \vec{W} with (\ref{eq:RDA.W}).
\item Find the residual ordination of unconstrained component by
  peforming SVD of $\vec{Y} - \vec{\hat Y} = \vec{U_r \bSigma_r V'_r}$.
\end{enumerate}
Direct implementation is:
<<echo=false>>=
dump("RDA", file="")
@

\subsubsection{Partial Analysis}

\Citet{terBraak86} introduced partial RDA (pRDA) where the effects of
some background variables, called \emph{covariates} are removed before
analysis with the interesting variables. This gives as the following
sequence of models:
\begin{eqnarray}
  \vec{\hat Y_z} &=& \vec{Z} (\vec{Z' Z})^{-1} \vec{Z' Y} \\
  \vec{\hat Y} &=& \vec{[X,Z]} (\vec{[X,Z]' [X,Z]})^{-1} \vec{[X,Z]' (Y- \hat Y_z)}\\
  \vec{Y_r} &=& \vec{Y - \hat Y - \hat Y_z} \;,
\end{eqnarray}
where \vec{Z} are the centred model matrix of conditions partialled
out, and \vec{[X,Z]} is combined model matrix formed by adding columns
of constraints \vec{X} and conditions \vec{Z} together. It is crucial
to combine conditions and constraints into same model matrix. Although
the direct effect of conditions was removed in the first step, they
may interact with with \vec{X}. The rule is that the residual
component should be the same when \vec{Z} are used as conditions and
\vec{X} as constrains, and when they both together are used as
constraints. SVD of \vec{\hat Y} gives the constrained ordination, and
the analysis of \vec{Y_r} gives the residual ordination.

The following implementation allows omitting either conditions or
constraints or both so that the function can be used for pRDA, RDA,
pPCA (= PCA after removing background variables), or usual PCA. Vegan
function \texttt{rda} has the same general behaviour. Although the
model equations use matrix crossproducts and their inverses, the code
uses QR decomposition.
<<echo=false>>=
dump("pRDA", file="")
@

\subsubsection{Constrained Correspondence Analysis}

Constrained Correspondence Analysis or Canonical Correspondence
Analysis is a parallel generalization of RDA in the CA framwork.  The
difference to RDA is that we need to perform Chi-square transformation
(\ref{eq:CAstand}) and juggle with weights in estimating.
The following overview of the algorithm only discusses
the constrained stage of CCA: partial and residual cases work
similarly and in parallel to RDA.

\begin{enumerate}
\item Let \vec{Y} be the response data matrix standardized to unit
  sum, \vec{r} and \vec{c} its row and column sums.
\item Perform the Chi-square standardization of \vec{Y} matrix with
  (\ref{eq:CAstand}).
\item Centre model matrix \vec{X} using row weights \vec{r}, and then weight the centred \vec{X} as $\vec{D_r}^{1/2} \vec{X}$.
\item Find the QR decomposition of weight-centred and weighted \vec{X}
  and with that the fitted values \vec{\hat Y}.
\item Find SVD of $\vec{\hat Y} = \vec{U \bSigma V'}$.
\item Find the WA scores for rows as $\vec{W} = \vec{D_r}^{-1/2} \vec{Y V} \bSigma^{-1}$.
\item De-weight column and row scores as $\vec{D_r}^{-1/2} \vec{U}$ and
  $\vec{D_c}^{-1/2} \vec{V}$.
\end{enumerate}
<<echo=false>>=
dump("pCCA", file="")
@

\subsubsection{Distance-based Redundancy Analysis}

Distance-based Redundancy Analysis (dbRDA) is related to RDA like PCoA
is related to PCA. The dependent data will be a matrix of distances or
dissimilarities, but we find the row scores with PCoA and use them as
a dependent data in the ordinary RDA.  The row scores must be scaled
by the square roots of the eigenvalues so that they correctly reflect
the importance of each dimension, and the scores correctly estimate
the variance of the data. Therefore we take only the positive
eigenvalues. In non-Euclidean, semimetric indices this means that
there are typically fewer than $n-1$ row vectors for $n$ rows, and the
eigenvalue scaled vectors do not exactly estimate the original
variation in the data.

The implementation calls \texttt{PCoA} and \texttt{dbRDA} so that this
allows partial analysis:
<<echo=false>>=
dump("dbRDA", file = "")
@ 

The function returns the \texttt{RDA} result object. However, the
scores do not have quite their usual meaning: columns are not species,
but they are scaled eigenvectors. There is no way of getting species
scores without original data matrix, because distance matrix has no
memory of species that were used in its calculation.  If Euclidean
distances were used, the results are similar to RDA (apart from
possible sign reversals) except for scores \vec{V} in constrained and
residual unconstrained components.
\endinput
