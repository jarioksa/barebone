<<echo=false,results=hide>>=
require(barebone)
require(vegan)
options(width=66)
@
\chapter{R Programming Interface to Multivariate Analysis}
\label{sec:algorithms}

This appendix presents barebone functions to perform basic ordination
analysis. The standard R functions use the same algorithms, but they
also have commands to check the input, handle common error situations
and exceptional cases and to process the results. All these features
make the functions usable, but they hide the plain core of the
methods. This appendix has nothing user friendliness.  The purpose is
only to show the minimal code of writing matrix algebra into R code.

\section{Matrix Algebra in R}
\label{sec:alg:matrixalgebra}

R is not designed primarily for matrix algebra. For instance,
multplication of two matrices \texttt{A * B} will multiply
elementwise. However, matrix operations are available, but they need a
longer notation: Matrix multiplication is given as \texttt{A \%*\% B}.  The most commonly needed matrix operations are given in Table \ref{tab:matrix}.
\begin{table}[b]
  \centering
   \caption{Examples of basic matrix operations in R. Uppercase letters refer to matrices and lower case to vectors.}
  \label{tab:matrix}
 \begin{tabular}{lcl}
\hline\noalign{\smallskip}
  Operation & Matrix & R \\
\noalign{\smallskip}\svhline\noalign{\smallskip}
  Transpose & \vec{A'} & \texttt{t(A)}\\
  Vector inner product & \vec{x'x} & \texttt{crossprod(x)} \\
  Vector outer product & \vec{xy'} & \texttt{outer(x, y)} \\
  && \texttt{x\%o\%y} \\
  Cross product & \vec{A'B} & \texttt{t(A) \%*\% B} \\
  && \texttt{crossprod(A, B)}\\
   Identity matrix $5 \times 5$ & $\vec{I}_n$ & \texttt{diag(5)} \\
   Multiply with diagonal matrix  & &
   \texttt{diag(x) \%*\% A} \\
   && \texttt{sweep(A, 1, x, "*")}\\
 Inverse & $\vec{A}^{-1}$ & \texttt{solve(A)}\\
\noalign{\smallskip}\hline\noalign{\smallskip}
 \end{tabular}
\end{table}

 There are alternative commands for many matrix operations, and some
 of these are shown in Table \ref{tab:matrix}. This chapter normally
 uses basic matrix operations to keep the R functions closer to their
 matrix presentations.

R provides powerful high-level matrix commands
based on LAPACK and LINPACK libraries \citep{LAPACK, LINPACK}. The
ordination methods can be very effectively programmed with these
high-level functions.

The basic ordination methods are all based on
matrix decomposition. Matrix decomposition is a powerful tool which
expresses the matrices in other forms which have some optimal
properties. The original data can be recovered from decomposition so
that nothing is destroyed, nothing is introduced, but things only are
expressed in a new way.  For instance, Principal Component Analysis
(PCA) is just singular value decomposition (SVD) with another name.

\subsection{Singular Value Decomposition}
\label{sec:alg:svd}

\index{singular value decomposition}
Singular value decomposition of $n \times m$ matrix \vec{X} is
\begin{equation}
  \label{eq:svd}
  \vec{X} = \vec{U \bSigma V'}\;,
\end{equation}
where \vec{U} and \vec{V} are orthonormal matrices and \vec{\bSigma}
is a diagonal matrix of \emph{singular values} \citep{Mard79}.
Orthonormality means that all columns of \vec{U} and \vec{V} are
orthogonal, and their sum of squares is 1, or $\vec{U'U} = \vec{I}$
and $\vec{V'V} = \vec{I}$. The importance of axes is conveyed in
singular values. The singular values are ordered by their magnitude,
and for an $n \times m$ matrix there may be $r = \min(n, m)$ positive
singular values. The number of the positive singular values gives the
rank of the matrix, or the number of independent components needed to
recover the original data. Basically, the rank is equal to the number
of observation (rows) $n$, but if the number of variables $m < n$ the
rank is reduced to $m$. If the data are standardized (like usually)
the rank may be further reduced. For a centred data, the rank is
reduced by one because the mean is fixed. If some of the variables can
be expressed as linear combinations of other variables, the rank is
reduced still further, and we speak of reduced or deficit rank modes.

\index{singular value decomposition}
The original data can be recovered from the data. The matrix equation
(\ref{eq:svd}) can be expressed in indexed form as:
\begin{equation}
  \label{eq:svdopen}
  x_{ij} = \sum_{k=1}^r u_{ik} \sigma_{k} v_{jk} \;.
\end{equation}
The singular values are ordered $\sigma_1 \geq \sigma_2 \geq \dots
\geq \sigma_r$ so that the first columns of \vec{U} and \vec{V} are
most important in (\ref{eq:svdopen}). The first columns give a
least squares of the original data.

\index{singular value decomposition}
Function \texttt{svd} performs the singular value decomposition. In
the following example we centre the data before the decomposition:

<<>>=
data(dune)
str(svd(scale(dune, scale=FALSE)))
@
The function returns singular values in vector \texttt{d} and row
and column scores in matrices \texttt{v} and \texttt{u}.

\subsection{Spectral Decomposition}
\label{sec:alg:spectral}

\index{spectral decomposition}\index{eigenvector decomposition}
Spectral decomposition \citep{Mard79}, also known as eigenvector
decomposition, is used for a symmetric matrix (in prinicple, spectral
decomposition also applies for nonsymmetric matrices, but it is not
used for those in ordination). Ecological data matrices are
nonsymmetric, and spectral decomposition is performed for their
symmetric crossproducts:
\begin{equation}
  \label{eq:eigen}
  \vec{X'X} = \vec{V \bLambda V'}\;.
\end{equation}
Here \vec{V} are orthonormal scores which are identical to \vec{V} of
(\ref{eq:svd}) (apart from possible sign reversals), and
\vec{\bLambda} is a diagonal matrix of eigenvalues which are squares
of singular values $\vec{\bLambda} = \vec{\bSigma}^2$. There are no
row scores, but they can be found indirectly after the analysis with
the help of original \vec{X}.

\index{spectral decomposition}\index{eigenvector decomposition}
Eigenvector decomposition looks like a more complicated and indirect
alternative to singular value decomposition. However, it is computationally
somewhat lighter and easier to implement, and earlier was the
standard method of matrix decomposition.  Therefore the ordination
results usually are called eigenvectors and eigenvalues, although they
were found with SVD.

\index{spectral decomposition}\index{eigenvector decomposition}
If \vec{X} was only centred, the spectral decomposition analyses a
matrix of crossproducts and sums of squares. However, often the
crossproducts are expressed as $(n-1)^{-1} \vec{\bar X' \bar X}$.
In that case the centred data will be equal to variances, and the
eigenvalues sum up to total variance.

\index{spectral decomposition}\index{eigenvector decomposition}
Function \texttt{eigen} performs the spectral decomposition.  We can
tell the function that data are symmetric so that it will not use time
to inspect input for symmetry. In the following, we analyse a centred
crossproduct matrix:

<<>>=
str(eigen(crossprod(scale(dune, scale=FALSE)), symmetric=TRUE))
@
The function returns eigenvalues in vector \texttt{values} and
eigenvectors in \texttt{vectors}.  The \texttt{values} are squares of
\texttt{d} of \texttt{svd}, and the \texttt{vectors} are similar (up
to sign) to \texttt{v} of \texttt{svd}.

\subsection{QR Decomposition}
\label{sec:QR}

\index{QR decomposition}
QR decomposition of matrix \vec{X} is
\begin{equation}
  \vec{X} = \vec{QR} \;,
\end{equation}
where \vec{Q} is a an orthonormal matrix and \vec{R} is an upper
triangular matrix.  The QR decomposition is not directly used in
ordination, but it has an important position in linear models and
hence in constrained ordination \citep{BlueBook}.

\index{QR decomposition}
The regression coefficient $\beta$ in a simple linear model $\hat y_i
= \beta x_i + \alpha$ can be expressed in two equivalent forms:
\begin{eqnarray}
  \beta &=& \frac{\sum_{i=1}^n x_i y_i}{\sum_{i=1}^n x_i^2}\\
  &=& (\vec{x'x})^{-1} \vec{x'y} \;.
\end{eqnarray}
Multivariate generalization of the vector form is obvious:
\begin{equation}
  \label{eq:B}
  \vec{B} = (\vec{X'X})^{-1} \vec{X'Y}\;.
\end{equation}
The model can be generalized to several predictor variables $\vec{X}$
and even to several dependent variables \vec{Y}.  The \emph{model matrix}
\vec{X}  can contain a column of ones to estimate the
intercept, and factors can be expressed as contrast variables.
However, in ordination models we deal with centred data and no
intercept is needed. The fitted values $\vec{\hat Y}$ are found with
\begin{eqnarray}
 \vec{\hat Y} &=& \vec{XB} \\
  \label{eq:Yhat}
&=& \vec{X} (\vec{X'X})^{-1} \vec{X'Y}\;.
\end{eqnarray}
Here $\vec{X} (\vec{X'X})^{-1} \vec{X'}$ is called the hat matrix
because it ``puts hat on \vec{Y}''. Hat matrix is also important in
further analysis of linear models, such as in influence measures
\citep{BlueBook}.

\index{QR decomposition}
It is easy to write (\ref{eq:Yhat}) in R:

<<eval=false>>=
Yhat <- X %*% solve(t(X) %*% X) %*% t(X) %*% Y
@
However, this is computationally problematic.  The problem is that
(\ref{eq:Yhat}) involves matrix inversion (\texttt{solve()}), and the
model matrix \vec{X} can be singular or nearly singular. With
completely singular matrices, we may use generalized inverses such as
function \texttt{ginv} in MASS \citep{MASS}, but there is a real
danger of loss of numerical precision with nearly singular matrices.

\index{QR decomposition}
Enter QR decomposition: (\ref{eq:B}) can be estimated from
\begin{equation}
  \vec{RB} = \vec{Q'Y}\;,
\end{equation}
which is easy to solve because \vec{R} is upper triangular.  There is
no loss of precision in QR decomposition, and even ill-conditioned and
rank-deficit models can be estimated.

\index{QR decomposition}
The usage of QR decomposition is very simple in R. First we make the QR
decomposition with function \texttt{qr}, and then we can extract fitted
values, residuals, regression coefficients etc:

<<>>=
data(varechem)
data(varespec)
Q <- qr(varechem)
Yhat <- qr.fitted(Q, as.matrix(varespec))
Yres <- qr.resid(Q, as.matrix(varespec))
beta <- qr.coef(Q, as.matrix(varespec))
@

\section{R Programming of Ordination Methods}

The unconstrained ordination methods are performed either with
singular value or spectral decomposition. Here we describe only
eigenvector methods which can be easily solved with R matrix library.

\subsection{Unconstrained Ordination}

Principal component analysis, Principal coordinate analysis and correspondence analysis are the major methods discussed in this chapter.

\subsubsection{Principal Component Analysis (PCA)}
\label{sec:alg:PCA}

\index{principal component analysis}
The simplest and numerically best method for PCA is equivalent to
singular value decomposition (SVD).  We perform SVD for centred data
matrix $\vec{\bar Y} = \vec{U \bSigma V'}$ (see \ref{eq:svd}).  This
gives us singular values which are square roots of eigenvalues
$\vec{\bSigma = \bLambda}^{1/2}$, and \vec{U} and \vec{V} are
orthonormal matrices of row and column scores.  The implementation is
straightforward:

<<echo=false>>=
dump("PCA", file = "")
@
The function returns a result of the R function \texttt{svd}. The only
optional argument is \texttt{scale} which defines how data is scaled
before the analysis. In general, the standard R function
\texttt{scale} transforms data as:
\begin{equation}
  \label{eq:PCAstand}
  \bar y_{ij} = \frac{y_{ij} - z_j}{s_j}\;,
\end{equation}
where $z_j$ is the column centre, and $s_j$ is the column
scale. Typically $z_j$ is the column average. With \texttt{scale =
  FALSE}, $s=1$ (or no scaling), and with \texttt{scale = TRUE}, $s_j$
are the variable (column) standard deviations. If data are scaled by
column standard deviations, all columns will have equal weight.  The
\texttt{scale} can also be a vector of scales. Using a constant
scale for all columns does not influence scores \vec{U} or \vec{V},
but it will influence the absolute singular values
\vec{\bSigma}---the proportions of individual elements $\sigma$ do
not change.  With the basic analysis, the singular values decompose
the original observed values, so that the sum of $\vec{\bSigma}^2$
equals the sum of $\vec{\bar Y}^2$. If we want the
singular values to decompose the variance of the data, we can call the
function as

<<>>=
data(dune)
str(PCA(dune, scale = rep(sqrt(nrow(dune)-1), ncol(dune))))
@
This divides columns with $\sqrt{n-1}$ so that the sum of
$\vec{\bSigma}^2$ equals the sum of variances of \vec{Y}, and the
singular values are standard deviations of the axes.  Using scale
$\sqrt{n}$ would give the same results with biased esimate of variance.

\paragraph{Spectral Decomposition}
\index{principal component analysis}
\index{singular value decomposition}
SVD is the preferred method of performing PCA, but earlier PCA was
usually performed as spectral decomposition of symmetric crossproduct
matrix:
\begin{equation}
  \vec{\bar Y' \bar Y} = \vec{V \bLambda V'}\;,
\end{equation}
where \vec{\bLambda} is a diagonal matrix of eigenvalues. The row
scores can be found as
\begin{equation}
  \label{eq:PCAu}
  \vec{U} = \vec{\bar Y' V \bLambda}^{-1/2}\;.
\end{equation}
The row and column scores \vec{U} and \vec{V} are identical to those
found in the previous methods (apart from possible sign reversals),
and $\vec{\bLambda} = \vec{\bSigma}^2$.

<<echo=false>>=
dump("PCAeig", file = "")
@
It is a common practice to omit multiplication with
$\vec{\bLambda}^{-1/2}$ in (\ref{eq:PCAu}) or, equivalently, division
by square roots of eigenvalues in \texttt{PCAeig}. In that case the
row scores \vec{U} are not orthonormal, but the sum of squares of each
column is proportional to square roots of eigenvalues. The equations
are:
\begin{eqnarray}
  \label{eq:PCAurot}
  \vec{U} &=& \vec{\bar Y' V}\\
  \vec{U'U} &=& \vec{\bLambda}^{1/2} \;.
\end{eqnarray}
With this scaling, \vec{U} are called \emph{scores matrix}, and
orthonormal \vec{V} are called \emph{loadings} or \emph{rotation
  matrix}: Multiplication with (\ref{eq:PCAurot}) rotates
\vec{\bar Y} to the principal components.

\index{principal component analysis}
\index{spectral decomposition}
Function \texttt{PCAeig} used crossproduct \vec{\bar Y' \bar Y} to get
the symmetric matrix for spectral decomposition. Most PCA software use
instead covariance matrix. This is equivalent of finding the
crossproduct of centred \vec{\bar Y} as $ (n-1)^{-1} \vec{\bar Y' \bar Y}$.
In this way the eigenvalues sum up to total
variance. Similarly, the scaled solution is usually found using
correlation matrix instead of explicit scaling of response data with
(\ref{eq:PCAstand}) like in SVD.

\paragraph{Power Method}
\label{sec:alg:PCA:power}
\index{principal component analysis!power method} Finally, we describe power
algorithm to find the first principal component and its
eigenvalue. Power algorithm is not recommended, but it is explained
here because it is so simple. The power algorithm is based on theorem
that any matrix multiplied with a high power of a matrix will converge
to the first eigenvector of that matrix. The high power of matrix is
achieved by multiplying a vector several time with a matrix. We take a
random vector \vec{u^*} and perform cycles $\vec{v^*} = \vec{\bar Y
  u^*}$, $\vec{u^*} = \vec{\bar Y' v^*}$ so long that $\vec{u^*}$
stabilizes and converges to an eigenvector. The matrix and vector
multiplication is equivalent of finding weighted sums, and therefore
this algorithm is also called weighted sums method \citep{Jong87}. At
every multiplication, the scale (sum of squares) of the result vector
will change. The first eigenvalue of the matrix gives the magnitude of
this change of scale \citep[cf.][]{Stev09}. Conversely, we can use the
change of sum of squares to estimate the eigenvalue, and stop the
iteratios when the eigenvalue stabilized in iteration. The following
function implements this method:

<<echo=false>>=
dump("PCApot1", file="")
@
The function returns normalized column scores $\vec{v}_1$ and row
scores $\vec{u}_1$ scaled by the eigenvalue, or $\sum_i v_{i1}^2 = 1$
and $\sum_i u_{i1}^2 = \lambda_1$.

It is possible to find more than one axis by orthogonalizing against
previous axes \citep{Jong87}. However, this is not worthwhile since
previous methods are better when more axes are needed. The method is
prone to accumulation of numerical errors, in particular for later,
orthogonalized axes. The method is only shown here because of its
simplicity.

\paragraph{R functions for PCA}
\index{principal component analysis}
R functions are based either on SVD or on spectral decomposition:
\begin{itemize}
\item \icode{prcomp} uses SVD, but it scales singular values by
  $\sqrt{n-1}$, and calls them standard deviations
  (\texttt{sdev}). Function also discards the orthonormal row scores
  \vec{U} that SVD returned, and instead uses (\ref{eq:PCAurot}) to
  find row scores scaled by eigenvalues (called \texttt{x}). The
  orthonormal column scores \vec{V} are called rotation.  The scaled
  solution is found by scaling the respone matrix with
  (\ref{eq:PCAstand}).
\item \icode{princomp} uses eigen decomposition of covariance or
  correlation matrices. The function uses biased estimate of variance,
  so that the denominator in variance is $n$ instead of $n-1$ of
  \texttt{prcomp}. The function returns square roots of eigenvalues
  and calls them standard deviations, just like in
  \icode{prcomp}. However, the reported eigenvalues differ from
  \icode{prcomp} which uses unbiased variance instead of biased
  variance of \icode{princomp}.  The function uses (\ref{eq:PCAurot})
  to find row scores \vec{U}. The row scores are called
  \texttt{scores} and orthonormal column scores are called
  \texttt{loadings}.
\item \icode{rda} in vegan package uses SVD in its PCA. The function
  returns singular values divided with $\sqrt{n-1}$ consistently with
  \icode{prcomp}.  Function returns orthonormal \vec{U} and
  \vec{V}. These are accessed via \texttt{scores} function  which
  scales them for
  functions like \texttt{plot} and \texttt{summary}.
  Vegan allows scaling either \vec{U} or \vec{V} by
  eigenvalues. In addition, there is a general scaling constant that
  tries to scale \vec{U} and \vec{V} so that they can be displayed in
  the same plot with equal scaling. Vegan includes a vignette on
  ``Design decisions'' which explains the scaling alternatives.
\end{itemize}

\subsubsection{Principal Coordinate Analysis (PCoA)}
\label{sec:alg:PCoA}

\index{principal coordinate analysis}
Principal Coordinate Analysis (PCoA) is closely related to PCA, in
particular to PCA performed through spectral decomposition (function
\texttt{PCAeig}).  The PCA with spectral decomposition analyses matrix
of crossproducts and sums of squares, but PCoA analyses distances.
The only task is to convert distances into crossproducts,
and then analyse these like in \texttt{PCAeig} \citep{Gower66,Mard79}. If
the distances are Euclidean, then PCoA gives the same results as PCA,
but is less efficient.  Normally we ignore the assumption of Euclidean
distances, and use PCoA with other distances or dissimilarities that
we regard as more appropriate for our data.

\index{principal coordinate analysis}
The algorithm follows \citet{Mard79}:
\begin{enumerate}
  \item Centre squared distances by rows and by columns
    \begin{equation}
      \vec{M} = \vec{H D}^2 \vec{H}\;,
    \end{equation}
    where \vec{D} are distances, and \vec{H} is a centring matrix. The
    centring matrix is defined as $\vec{H} = \vec{I}-n^{-1}\vec{11'}$,
    where $n$ in the number of rows and columns of square \vec{D},
    \vec{I} is an identity matrix, and \vec{1} is a column vector of
    ones.  The centring matrix gives a mathematically nice
    presentation of centring, but in practice we use numerically more
    stable procedures. The example code below uses \texttt{scale}
    function, and the standard R implementation of the method
    (function \icode{cmdscale}) has a fast C routine \texttt{dblcen}
    which is the choice in real applications.
  \item Matrix $-\vec{M}/2$ presents the crossproducts and sum of
    squares and is submitted to spectral decomposition.
\end{enumerate}

<<echo=false>>=
dump("PCoA", file = "")
@

\index{principal coordinate analysis}
The function returns all non-zero eigenvalues and orthonormal row
scores. Due to centring, there are at most $n-1$ positive
eigenvalues. If \vec{D} was non-Euclidean, semimetric index, there
will also be negative eigenvalues, so that the number of positive
eigenvalues is even lower than $n-1$ \citep{Gower85}. Negative
eigenvalues are not removed in the current function, nor are any
warnings issued. If we wish to scale the row scores by eigenvalues, we
must use only the positive ones.

\index{principal coordinate analysis}
This algorithm is used in standard R function \icode{cmdscale}.
Functions like \icode{adonis} and \icode{betadisper} also use
basically the same kind of translation of dissimilarities to
crossproducts.

\subsubsection{Weighted Principal Coordinate Analysis}
\label{sec:alg:wPCoA}

\index{principal coordinate analysis}
Any eigenvector method can be weighted, but here we only demonstrate
the weigthing for PCoA.  Similar principles also hold for PCA or RDA
(and CA always is inherently weighted).  Integer weights may only tell
how many copies of a row we have in our data set: weight 1 means a
unique row, and weight 2 indicates a duplicated row. However, the
weights can also be non-integer.

\index{principal coordinate analysis}
Weighting reduces or increases the influence of a point with respect
to other points. The influence of the point in unweighted ordination
is given by its distance from the centroid: far points have a higher
moment. Weighting is moving points further from or closer to the
centroid.  After weigthing, we perform the rotation via SVD or
spectral decomposition. After rotation we have to recover the original
configuration and move the points back to their original positions:
weighting influences the rotation but not the full space.  The stages
of weighted ordinations are weigthing, rotation, and de-weigthing.

\index{principal coordinate analysis}
Weighted PCoA has the following stages:
\begin{enumerate}
\item Let \vec{D} be a $n \times n$ distance matrix, and \vec{w} a
  corresponding weight vector of length $n$.  Centre first rows and
  then columns of this matrix using weighted centres with weights
  $\vec{w}^{1/2}$ to get a double centred matrix \vec{\bar{D}}.
\item Weight this matrix
  \begin{equation}
    \vec{M} = \vec{W}^{1/2} \vec{\bar D} \vec{W}^{1/2}\;,
  \end{equation}
  where \vec{W} is a diagonal matrix of weights \vec{w}.
\item Perform the spectral decomposition
  \begin{equation}
    -\vec{M}/2 = \vec{\tilde U} \bLambda \vec{\tilde U'}\;,
  \end{equation}
  where \vec{\tilde U} are the orthonormal row scores, and
  \vec{\bLambda} is a diagonal matrix of eigenvalues.
\item De-weight or remove the weights by division of row scores
  \begin{equation}
    \vec{U} = \vec{W}^{-1/2} \vec{\tilde U}\;.
  \end{equation}
\end{enumerate}

\index{principal coordinate analysis}
The implementation is straighforward:
<<echo=false>>=
dump("wPCoA", file = "")
@
Function \icode{wcmdscale} in vegan follows this algorithm.

\subsubsection{Correspondence Analysis (CA)}
\label{sec:alg:CA}

\index{correspondence analysis}
Correspondence analysis (CA) is weighted PCA with Chi-square metric
\index{distance!Chi-square}
\citep{Nish80,Gree84, GreeHast87, Gree07}. Just like with
PCA, we can use SVD or spectral decomposition.

\paragraph{Singular Value Decomposition}
\label{sec:alg:CA:svd}
\index{correspondence analysis}
\index{singular value decomposition}
The SVD algorithm is based on \citet{GreeHast87}:
\begin{enumerate}
\item Let \vec{Y} be the response matrix standardized to unit sum and
  \vec{r} and \vec{c} vectors of its row and column sums.  Perform
  weighted Chi-square transformation, where a typical element
  $\{\bar{y}_{ij}\} = \vec{\bar Y}$ is
  \begin{equation}
    \label{eq:CAstand}
    \bar{y}_{ij} = \frac{y_{ij} - r_i c_j}{\sqrt{r_i c_j}} \;.
  \end{equation}
  This is obviously \indexrev{Chi-square}{transformation} because by usual
  definition $\chi^2 = \sum \bar{y}_{ij}^2$.
\item Perform SVD  of $\vec{\bar Y} = \vec{\tilde{U} \bSigma
    \tilde{V}'}$.
\item De-weight the solution by
  \begin{eqnarray}
    \vec{U}&=& \vec{R}^{-1/2} \vec{\tilde{U}}\\
    \vec{V}&=& \vec{C}^{-1/2} \vec{\tilde{V}} \;,
  \end{eqnarray}
  where $\vec{R}$ and $\vec{C}$ are diagonal matrices of row and
  column weights \vec{r} and \vec{c}.
\end{enumerate}
Direct implementation in R is:
<<echo=false>>=
dump("CA", file = "")
@
This returns a result from SVD with row and column scores (\vec{V} and
\vec{U}) and singular values which are square roots of eigenvalues.

\paragraph{Spectral Decomposition}
\label{sec:alg:CA:eig}
\index{correspondence analysis}
\index{spectral decomposition}
The spectral decomposition of a symmetric matrix follows
\citet{Nish80}:
\begin{enumerate}
\item Let \vec{Y} be the response matrix standardized to unit sum and
  \vec{r} and \vec{c} vectors of its row and column sums. Find the
  weighted crossproduct matrix
  \begin{equation}
    \label{eq:CAmattrans}
    \vec{\bar Y' \bar Y} = \vec{C}^{-1/2} \vec{Y'} \vec{R}^{-1} \vec{Y} \vec{C}^{-1/2} \;,
  \end{equation}
  where $\vec{C}$ and $\vec{R}$ are diagonal matrices of column
  and row sums.
\item Perform spectral decomposition:
  \begin{equation}
    \label{eq:CAeigen}
    \vec{\bar Y' \bar Y} = \vec{\tilde V \bLambda \tilde V'} \;.
  \end{equation}
\item De-weight the column scores:
  \begin{equation}
    \vec{V} = \vec{C}^{-1/2} \vec{\tilde{V}} \;.
  \end{equation}
\item Find the row scores:
  \begin{equation}
    \vec{U} = \vec{R}^{-1} \vec{ Y V} \vec{\bLambda}^{-1/2} \;.
  \end{equation}
\item The original transformation (\ref{eq:CAmattrans}) was not
  centred, and the first eigen vector of (\ref{eq:CAeigen}) will be
  so called trivial solution of $\lambda=1$ to do the centring omitted
  earlier. We wish to discard this first trivial axis.
\end{enumerate}
Direct implementation in R is:
<<echo=false>>=
dump("CAeig", file="")
@

\paragraph{Weighted Principal Coordinates}
\label{sec:alg:CA:wPCoA}
\index{correspondence analysis}
Correspondence Analysis is a distance-based method, like all other
eigenvector methods. There are two differences to PCA: Correspondence
analysis preserves Chi-square distances while PCA preserves Euclidean
distance, and CA is weighted while PCA is unweighted.  Chi-square
distances are Euclidean distances of Chi-square standardized data
\citep{LegeGall01}. The following algorithm combines Chi-square
distances with weighted Principal Coordinate Analysis
(\texttt{wPCoA}):
\begin{enumerate}
\item Let \vec{Y} be the response matrix standardized to unit sum,
  \vec{r} and \vec{c} its row and column sums with corresponging
  diagonal matrices \vec{R} and \vec{C}. Calculate the Chi-square
  transformed data \vec{\bar Y}:
  \begin{equation}
    \vec{\bar Y} = \vec{R}^{-1} \vec{Y} \vec{C}^{-1/2} \;.
  \end{equation}
\item Calculate the Euclidean distances of \vec{\bar Y}. They will be
  the Chi-square distances.
\item Find the weighted PCoA of these distances using \vec{r} as
  weights. This will have eigenvalues \vec{\bLambda} and weighted
  orthonormal row scores \vec{U}.
\item The column scores \vec{V} cannot be found directly from the
  distance matrix, but they can be found with \vec{Y} or \vec{\bar Y}:
  \begin{eqnarray}
    \vec{V} &=& \vec{C}^{-1} \vec{Y' U} \bLambda^{-1/2}\\
    &=& \vec{C}^{-1/2} \vec{\bar{Y}' R U} \bLambda^{-1/2}\;.
  \end{eqnarray}
\end{enumerate}
The code below uses the firs alternative:

<<echo=false>>=
dump("CAdist", file="")
@

\paragraph{Reciprocal Averaging}
\label{sec:alg:CA:RA}
\index{reciprocal averaging} \index{correspondence analysis!reciprocal
  averaging} \index{weighted averages} The power method
(\pageref{sec:alg:PCA:power}) can be also used to find the first axis
of CA. The only difference is that instead of weighted sums we use
weighted averages. The power method is historically interesting
because CA was introduced to ecologists as a method of cyclic weighted
averages under the name of Reciprocal Averaging \citep{Hill73}.
Because of the simplicity of the algorithm, CA (as reciprocal
averaging) was regarded as simpler method than mathematically more
complicated PCA. Similar simple power algorithm can be used for the
first axis of PCA (\pageref{sec:alg:PCA:power}), but even in that
time, computer programs used more effective but mathematically more
complicated algorithms. Similary as with PCA, the eigenvalue can be
expressed the ratio of change of scales between successive iterations,
and the iterations can be stopped when the ratio stabilized. In CA,
eigenvalue is always $\lambda_1 < 1$ so that the vectors shrink during
iterations.

\index{reciprocal averaging} \index{correspondence analysis!reciprocal
  averaging} \index{weighted averages} Instead of explicit matrix
equations, the following implementation uses function \icode{wascores}
of vegan for weighted averages. With argument \code{expand = TRUE}
this function retains the original scaling of vectors and expands the
scores. Moreover, it returns the rate of shrinkage as argument
\code{shinkage} which actually is the current estimate of
eigenvalue. With this function the implementation is very simple:

<<echo=false>>=
dump("CAwa1", file="")
@

\subsubsection{Nonmetric Multidimensional Scaling}
\label{sec:alg:NMDS}

Nonmetric multidimensional scaling (NMDS) is defined by
isotonic\footnote{\citet{Krus64,Krus64num} used the name monotonic
  regression, but isotonic regression is now the standard name
  \citep{BarlEtal72}.}  regression and goodness of fit measure
\citep{Krus64, Krus64num}. The goodness of fit measure is called
stress $S$ and it is defined as
\begin{equation}
  S = \sqrt{\frac{S^*}{T^*}} = \sqrt{\frac{\sum (d_{ij} - \hat{d}_{ij})^2}{\sum d_{ij}^2}}\;,
\end{equation}
where $d$ are distances between points in the final solution, and
$\hat d$ are the fitted distances. Observed dissimilarities do not
appear in the equation for stress.  They influence the isotonic
regression that states that $\hat d$ have the same rank order as
observed dissimilarities \citep{Krus64, Krus64num, BarlEtal72}.

\citet{Krus64num} presented and algorithm for isotonic regression, but
standard R function \icode{isoreg} implements another algorithm of the
same method \citep{BarlEtal72}. The isotonic regression uses only rank
order information of observed dissimilarities.  Treatment of ties is
crucial for effective ordination \citep{Krus64,Krus64num}.  In
particular presence-absence or cover class data have several tied
values. In addition, there may be some falsely broken ties, because of
floating point precision issues in binary computers.  A particularly
serious case of ties are dissimilarities of ones between sampling
units that share no species. If we keep the fitted values tied, the
ordination tries to put all these extreme sites at the same distance
from each other, resulting in spherical or ring like
configuration. \citet{Krus64, Krus64num} recommends allowing breaking
ties.  He calls this primary tie treatment.  Function \code{NMDS}
below follows this recommendation.  Function \icode{isoMDS} of the
MASS package \citep{MASS} does not treat ties properly, but breaks
them in order dependent way.

Only monotone regression and stress were defined, but not the way how
to minimize the function.  NMDS is a nonlinear optimization problem
where coordinates of points in ordination space are the
parameters. There are $nk$ parameters for $n$ points and $k$
dimensions which makes NMDS a hard problem. For effective optimization
we need information on the directions where points should be moved to
minimize stress. This information is provided by the derivative of
stress with respect to coordinate $x_{il}$ for point $i$ on dimension
$l$ \citep{Krus64num}:
\begin{equation}
 \frac{\D S}{\D x_{il}} = S \sum_{j=1}^n \left[ \left( \frac{d_{ij} - \hat{d}_{ij}}{S^*} - \frac{d_{ij}}{T^*}\right) \frac{u_{ik} - u_{jk}}{d_{ij}} \right]\;,
\end{equation}
where $u$ is the coordinate of point $i$ on dimension $k$. In
addition, we want to have a robust and effective optimization
routine. \citet{Krus64num} described the method of steepest descent,
or always moving along the gradient, but this is usually ineffective
for difficult tasks like NMDS and better methods should be used
\citep{Pres92, NoceWrig99}.  R function \icode{optim} has several good
alternatives, and \code{NMDS} below defaults to
Broyden-Fletcher-Goldfarb-Shanno algorithm as does \icode{isoMDS}
function in the MASS package \citep{MASS}.

The following implementation has only two major steps and two internal
functions:
\begin{enumerate}
\item Set starting values for iteration. The function uses random
  uniform coordinates if initial configuration (\code{x}) was not
  provided.  Most software use PCoA as default starting values, like
  does the \icode{isoMDS} which uses \icode{cmdscale}. This usually
  gives fast convergence to a solution, but may lead to a local
  minimum close to a metric solution.
\item Minimize the stress with \icode{optim}. Function allows
  selection of optimization method, and passes all extra arguments to
  \icode{optim}. Stress is defined in internal function \code{stress}
  and its gradient in \code{dstress}. The internal notation is taken
  from \icode{isoreg}, and \code{y} refers to $d$ and \code{yf} to
  $\hat d$.
\end{enumerate}

<<echo=false>>=
dump("NMDS", file = "")
@
The function returns the result of \icode{optim} where \code{par}
contain the final configuration and \code{value} the final
stress. Function \icode{isoMDS} in the MASS package \citep{MASS} uses
the same method (except for handling ties), but implemented in C.

\subsection{Constrained Ordination}
\label{sec:alg:constrained}

In constrained ordination we have constraints or environmental
variables that restrict the ordination. The two basic methods are
Redundancy Analysis which is generalized from PCA, and Constrained
Correspondence Analysis which is generalized from CA.

\subsubsection{Redundancy Analysis}
\label{sec:alg:RDA}

\index{redundancy analysis}
When \citet{terB86} introduced Constrained or Canonical
Correspondence Analysis, he also suggested an algorithm with
alternating regression and weighted averaging steps. In regression
step, the site scores are found as linear combinations of constraints
(environmental variables), and in weighted averaging step site scores
are found from species scores. Redundancy analysis is similar but
weighted averaging is replaced with weighted summation step
\citep{terB86, Jong87}. \citet{Palm93} gives a remarkably
lucid overview of this algorithm.

\index{redundancy analysis}
\index{singular value decomposition}
\index{QR decomposition}
The algorithm of \citet{terB86} is often taken as the definition of
the method of constrained ordination. However, we can achieve the same
results with some other algorithms. \citet{LegeLege98} showed that a
better way of obtaining all constrained axes simultaneously is to
analyse fitted values \vec{\hat Y} of
\begin{equation}
  \label{eq:RDA.Yhat}
  \vec{\hat Y} = \vec{X} (\vec{X'X})^{-1} \vec{X'Y} \;,
\end{equation}
where \vec{Y} is the centred response matrix, and \vec{X} is the
centred model matrix of constraints (environmental variables).  The
constrained ordination solution is SVD of \vec{\hat Y}.  Obviously,
(\ref{eq:RDA.Yhat}) is identical to the model of multiple linear
regression (\ref{eq:Yhat}). We can use QR decomposition
(p. \pageref{sec:QR}) to estimate the fitted values and residual
values to avoid the possible loss of precision in matrix inversion.

\index{redundancy analysis}
\index{singular value decomposition}
The SVD of (\ref{eq:RDA.Yhat})  finds the row scores \vec{U} that are linear
combinations of constraints and the corresponding column scores
\vec{V}.  The alternating regression and weighted sums algorithm also
finds row scores \vec{W} that are weighted sums of column scores. In
CA framework, they are weighted averages, and consequently called
weighted average or WA scores \citep{Palm93}. In direct regression
approach they must be estimated separately as
\begin{equation}
  \label{eq:RDA.W}
  \vec{W} = \vec{Y V \bSigma}^{-1}
\end{equation}

\index{redundancy analysis}
The analysis of \vec{\hat Y} gives the constrained part analysis. The
analysis of the residuals $\vec{Y} - \vec{\hat Y}$ gives the residual
unconstrained analysis.

\index{redundancy analysis}
\index{singular value decomposition}
Putting this all together gives the following algorithm:
\begin{enumerate}
\item Let \vec{Y} be centred response matrix, and \vec{X} centred
  model matrix of constraints.  Find QR decomposition of \vec{X}
  and fitted values \vec{\hat Y}.
\item Perform SVD of $\vec{\hat Y} = \vec{U \bSigma V'}$ with
  orthonormal column and row matrices \vec{U} and \vec{V}, and
  diagonal matrix of singular values \bSigma.
\item Find the ``WA scores'' \vec{W} with (\ref{eq:RDA.W}).
\item Find the residual ordination of unconstrained component by
  peforming SVD of $\vec{Y} - \vec{\hat Y} = \vec{U}_r \bSigma_r \vec{V'}_r$.
\end{enumerate}
Direct implementation is:
<<echo=false>>=
dump("RDA", file="")
@

\subsubsection{Partial Analysis}

\index{redundancy analysis!partial}
\Citet{terB86} introduced partial RDA (pRDA) where the effects of
some background variables, called \emph{covariates} are removed before
analysis with the interesting variables. This gives as the following
sequence of models:
\begin{eqnarray}
  \vec{\hat Y_z} &=& \vec{Z} (\vec{Z' Z})^{-1} \vec{Z' Y} \\
  \vec{\hat Y} &=& \vec{[X,Z]} (\vec{[X,Z]' [X,Z]})^{-1} \vec{[X,Z]' (Y- \hat Y_z)}\\
  \vec{Y_r} &=& \vec{Y - \hat Y - \hat Y_z} \;,
\end{eqnarray}
where \vec{Z} are the centred model matrix of conditions partialled
out, and \vec{[X,Z]} is combined model matrix formed by adding columns
of constraints \vec{X} and conditions \vec{Z} together. It is crucial
to combine conditions and constraints into same model matrix. Although
the direct effect of conditions was removed in the first step, they
may interact with with \vec{X}. The rule is that the residual
component should be the same when \vec{Z} are used as conditions and
\vec{X} as constrains, and when they both together are used as
constraints. SVD of \vec{\hat Y} gives the constrained ordination, and
the analysis of \vec{Y_r} gives the residual ordination.

\index{redundancy analysis!partial}
The following implementation allows omitting either conditions or
constraints or both so that the function can be used for pRDA, RDA,
pPCA (= PCA after removing background variables), or usual PCA. Vegan
function \icode{rda} has the same general behaviour.

<<echo=false>>=
dump("pRDA", file="")
@

\subsubsection{Constrained Correspondence Analysis (CCA)}

\index{constrained correspondence analysis}
Constrained Correspondence Analysis or Canonical Correspondence
Analysis is a parallel generalization of RDA in the CA framwork.  The
difference to RDA is that we need to perform \indexrev{Chi-square}{transformation}
(\ref{eq:CAstand}) and juggle with weights.
The following overview of the algorithm only discusses
the constrained stage of CCA: partial and residual cases work
similarly and in parallel to RDA.

\begin{enumerate}
\item Let \vec{Y} be the response data matrix standardized to unit
  sum, \vec{r} and \vec{c} its row and column sums.
\item Perform the Chi-square standardization of \vec{Y} matrix with
  (\ref{eq:CAstand}).
\item Centre model matrix \vec{X} using row weights \vec{r}, and then
  weight the centred \vec{X} as $\vec{R}^{1/2} \vec{X}$, where \vec{R}
  is a diagonal matrix of \vec{r}.
\item Find the QR decomposition of weight-centred and weighted \vec{X}
  and with that the fitted values \vec{\hat Y}.
\item Find SVD of $\vec{\hat Y} = \vec{U \bSigma V'}$.
\item Find the WA scores for rows as $\vec{W} = \vec{R}^{-1/2}
  \vec{Y V} \bSigma^{-1}$.
\item De-weight column and row scores as $\vec{R}^{-1/2} \vec{U}$ and
  $\vec{C}^{-1/2} \vec{V}$, where \vec{C} is a diagonal matrix of
  \vec{c}.
\end{enumerate}

<<echo=false>>=
dump("pCCA", file="")
@

\subsubsection{Distance-based Redundancy Analysis}

\index{redundancy analysis!distance based}
Distance-based Redundancy Analysis (dbRDA) is related to RDA like PCoA
is related to PCA \citep{LegeAnde99}.
The dependent data will be a matrix of distances or
dissimilarities, but we find the row scores with PCoA and use them as
a dependent data in the ordinary RDA.  The row scores must be scaled
by the square roots of the eigenvalues so that they correctly reflect
the importance of each dimension, and the scores correctly estimate
the variance of the data. Therefore we take only the positive
eigenvalues. In non-Euclidean, semimetric indices this means that
there are typically fewer than $n-1$ principal coordinate vectors for
$n$ rows.

\index{redundancy analysis!distance based}
The implementation calls \texttt{PCoA} and \texttt{dbRDA} so that this
allows partial analysis:

<<echo=false>>=
dump("dbRDA", file = "")
@

\index{redundancy analysis!distance based}
The function returns the \texttt{RDA} result object. However, the
scores do not have quite their usual meaning: columns are not species,
but they are scaled eigenvectors. There is no way of getting species
scores without original data matrix, because distance matrix has no
memory of species that were used in its calculation.  If Euclidean
distances were used, the results are similar to RDA (apart from
possible sign reversals) except for scores \vec{V} in constrained and
residual unconstrained components.

Function \icode{capscale} in vegan follows this algorithm. It also
adds species scores similarly as in PCA (\ref{eq:PCAu}) if original
data matrix \vec{Y} is available.

%%% The following chapter may be removed later, but for time being I
%%% put these things here.

\section{Prediction Methods for Data and Scores}

Ordination methods were based on matrix decomposition which means that
data can be recovered from the results. We also saw that scores can be
recovered from each other so that we can predict scores for new
observartions (rows) or columns (variables, species).

\subsection{Data Recovery}

\index{principal component analysis}
\index{correspondence analysis}
Uncostrained PCA and CA are both based on singular value decomposition
possibly with some preceding standardizations. The internal,
standardized form of the data \vec{\bar Y} can be recovered in the
same way in both:
\begin{equation}
  \label{eq:databack}
  \vec{\bar Y} = \vec{U} \bLambda^{1/2} \vec{V'}\;,
\end{equation}
which uses eigenvalues \vec{\bLambda} and orthonormal row and column
scores.  The methods differ only in the way they undo the
standardizations to find the original data. In PCA we must undo the
standardization of (\ref{eq:PCAstand}):
\begin{equation}
  y_{ij} = s_j \bar{y}_{ij} + z_j\;,
\end{equation}
and in CA we must undo (\ref{eq:CAstand}):
\begin{equation}
  y_{ij} = (\bar{y}_{ij} + 1) r_i c_j G\;,
\end{equation}
where $G$ is the grand total of $\vec{Y}$.

There is no way of estimating original observations \vec{Y} in PCoA
using arbitrary dissimilarities, but we can only recover the
\citet{Gower66} matrix $\vec{M}$ derived from the original
dissimilarities $\vec{D}$. If $\vec{D}$ was based on non-Euclidean
dissimilarities, some of the eigenvalues are negative and the
corresponding scaled eigenvectors are imaginary \citep{Gower85}. To
estimate the transformed data $\vec{M}$ the distances estimated from
imaginary axes must be subtracted from the distances estimated from
the real axes. For $r$ positive eigenvalues and real axes, and $s$
negative eigenvalues and imaginary eigenvalues (with $r+s = n-1$) this
gives us \citep{Gower85}:
\begin{equation}
  \bar{d}_{ii} + \bar{d}_{jj} - 2 \bar{d}_{ij} = m_{ij}^2 = \\
  \sum_{k=1}^r \lambda_k (u_{ik} - u_{jk})^2 - \sum_{k=r+1}^{r+s} |
  \lambda_k |  (u_{ik} - u_{jk})^2 \;,
\end{equation}
where $\bar{d}$ are double-centred dissimilarities
\citep{Gower66,Mard79}.

In constrained ordination we can recover the fitted values of the
constrained part and separately the residuals from the unconstrained
ordination. We can use (\ref{eq:databack}) to estimate the data.
Package vegan implement such procedures in \texttt{fitted} and
\texttt{residuals} functions. In the following model

<<>>=
data(varespec)
data(varechem)
mod <- rda(varespec ~ ., varechem)
@
the solutions of constrained and unconstrained components can be
recovered from

<<results=hide>>=
rda(fitted(mod))
rda(residuals(mod))
@

To recover the original data, we must add constrained and residual
components:
\begin{equation}
  \vec{Y} = \vec{U} \bLambda^{1/2} \vec{V'} + \vec{U}_r \vec{\bLambda}_r^{1/2} \vec{V'}_r\;,
\end{equation}
where the terms indexed with $r$ refer to unconstrained residual
terms, and unindexed to the constrained terms. This means that we must
use LC scores \vec{U} of constrained component for the recovery of
observed data. Package analogue uses WA scores \vec{W} for estimating
``residual lengths'' \citep[following][]{CANOCO}, but these do not
estimate precisely the original data.

\subsection{Reconstitution of Scores}

\index{prediction!ordination scores}
\index{prediction!principal component analysis}
Species scores \vec{V} and site scores \vec{U} are directly related to
each other in unconstrained ordination. In PCA the mutual translation
functions are:
\begin{eqnarray}
 \vec{V} &=& \vec{\bar Y'} \vec{U} \bLambda^{-1/2}\\
 \vec{U} &= & \vec{\bar Y} \vec{V} \bLambda^{-1/2}
\end{eqnarray}
The division by the squareroots of eigenvalue takes care of retaining
the original scaling of orthonormal scores. The equations include
centred data matrix \vec{\bar Y}, and supplying new data allows
finding scores for new observations.

\index{prediction!correspondence analysis}
\index{prediction!ordination scores}
Unconstrained CA has similar dual relationship, but in CA we must take
care of row totals \vec{r} and column totals \vec{c}. These are given in
diagonal matrices $\vec{R}$ and $\vec{C}$:
\begin{eqnarray}
\vec{V} &= \vec{C}^{-1/2} \vec{\bar Y'} \vec{R}^{1/2} \vec{U} \bLambda^{-1/2}\\
\vec{U} &= \vec{R}^{-1/2} \vec{\bar Y} \vec{C}^{1/2}\vec{V} \bLambda^{-1/2}
\end{eqnarray}
Supplying new \vec{Y} allows finding scores for new rows or
columns. The needed weights are also found from the supplied \vec{Y}.

\index{prediction!ordination scores}
\index{prediction!constrained ordination}
In constrained ordination the situation is more complicated, and the
duality of scores does not hold. Matrix \vec{U} is found from
(weighted) centred constraints \vec{\bar X} and regression
coefficients \vec{B}. The species scores \vec{V} are found from
\vec{U} similarly as above, but using symmetric formula for sites does
not give scores \vec{U}, but instead gives different scores \vec{W}:
\begin{eqnarray}
 \vec{U} &=& \vec{\bar X B}\\
 \vec{V} &=& \vec{C}^{-1/2} \vec{\bar Y'} \vec{R}^{1/2} \vec{U} \bLambda^{-1/2}\\
 \vec{W} &=& \vec{R}^{-1/2} \vec{\bar Y} \vec{C}^{1/2} \vec{V} \bLambda^{-1/2}
\end{eqnarray}
The equations for RDA are similar, but without weight terms \vec{R}
and \vec{C}.

\index{linear combination scores}
\index{weighted averages scores}
Scores \vec{U} are called linear combination or LC scores, and scores
\vec{W} are called weighted average or WA scores. To add points to the
solution, you must submit new environmental data \vec{X} for LC scores
\vec{U}, and new dependent data \vec{Y} for WA scores \vec{W} or
species scores \vec{V}.

In unconstrained analysis (PCA and CA), scores \vec{U} and \vec{V} are
symmetric so that you can derive \vec{U} from \vec{V} or \vec{V} from
\vec{U}, and you can repeat this cyclically. However, the scores of
constrained ordination (RDA and CCA) are nonsymmetric: you can only
derive \vec{V} from \vec{U}, but not vice versa. If you use the
symmetric formulae of unconstrained ordination for \vec{V}, you will
get WA scores \vec{W} which are not identical (or even similar) to LC
scores \vec{U}. Further, you cannot derive species scores \vec{V} from
WA scores \vec{W}, and there is no similar dual relationship as in
unconstrained ordination. The derivation of scores is from \vec{U} to
\vec{V} to \vec{W} but this route cannot be reversed.

\bibliographystyle{spbasic}
\bibliography{mmecor}
\endinput
